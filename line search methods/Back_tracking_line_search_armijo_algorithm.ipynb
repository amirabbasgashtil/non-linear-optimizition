{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHRxBqC7Nb8sAv8dfYmOMg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirabbasgashtil/non-linear-optimizition/blob/main/line%20search%20methods/Back_tracking_line_search_armijo_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back-tracking line search algorithm with armijo rule"
      ],
      "metadata": {
        "id": "IHyTOdqHdJ6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "c9SuQGiQm2qq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. define the function, calculate the gradient and Hessian\n",
        "\n",
        " for example we choose $f(x,y) = x^2 + y^2$"
      ],
      "metadata": {
        "id": "V2Rs5BkSdZas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_plot():\n",
        "  x = np.linspace(-10,10,1000)\n",
        "  y = f(x) # edit this\n",
        "\n",
        "  plt.plot(x,y)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Y6UdUiJ_vFWD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return x[0] ** 2 + x[1] ** 2"
      ],
      "metadata": {
        "id": "afN2jfnadWi0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_f(x):\n",
        "  x0 = x[0]\n",
        "  x1 = x[1]\n",
        "  return np.array([2 * x0, 2 * x1])"
      ],
      "metadata": {
        "id": "AIOGoAL3lpvW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Hessian_f(x):\n",
        "  return np.array([[2,0],[0,2]])"
      ],
      "metadata": {
        "id": "HhPn7XfpmU6K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Hessian_inverse(H):\n",
        "  return np.linalg.inv(H)"
      ],
      "metadata": {
        "id": "pg64jHnUoHLG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  the $\\phi$ of $\\alpha$ :\n",
        "\n",
        "  $\\phi(\\alpha) = f(x_k + \\alpha_k * p_k)$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "  the L of $\\alpha$ :\n",
        "\n",
        "  $L(\\alpha) = f(x_k) + c \\alpha_k \\nabla f^{T}(x_k)p_k$\n",
        "\n",
        "  which c is a scalar, $0<c<1$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "  armijo rule:\n",
        "\n",
        "  ϕ(α)≤L(α)"
      ],
      "metadata": {
        "id": "4FtC3qBbfvcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_alpha(x,p_k,alpha_k, beta, c):\n",
        "  is_find = False\n",
        "  while is_find is False:\n",
        "    if f(x + alpha_k * p_k) <= f(x) + c * alpha_k * np.dot(gradient_f(x), p_k):\n",
        "      is_find = True\n",
        "    else:\n",
        "      alpha_k = alpha_k * beta\n",
        "  return alpha_k"
      ],
      "metadata": {
        "id": "W_vAYIx3kCP0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def newtons_method(f, x0, convergence_thresh=0.12, beta=0.8, c=0.6, max_itrs=10**5, print_progress=True):\n",
        "  converged = False\n",
        "  itr_count = 0\n",
        "  x = x0\n",
        "  trajectory=[]\n",
        "  alpha = 1\n",
        "  while converged is False:\n",
        "    p_k = - gradient_f(x) / np.linalg.norm(gradient_f(x))\n",
        "    alpha = find_alpha(x, p_k, alpha, beta, c)\n",
        "    B = Hessian_inverse(Hessian_f(x))\n",
        "\n",
        "    itr_count += 1\n",
        "    trajectory.append(x)\n",
        "    x = x + alpha * B @ p_k\n",
        "\n",
        "    #convergence\n",
        "    if np.linalg.norm(gradient_f(x)) <= convergence_thresh:\n",
        "      converged = True\n",
        "      trajectory.append(x)\n",
        "      print('minimal solution : ',x)\n",
        "    if converged is False and itr_count > max_itrs:\n",
        "      converged = True\n",
        "      print('failed to converge.')\n",
        "    # output\n",
        "    if print_progress and itr_count <= 10:\n",
        "      print('iteration', itr_count, '\\nx:',x)\n",
        "      print('-----')\n",
        "  return x, np.array(trajectory)"
      ],
      "metadata": {
        "id": "DRexHOinJ4oO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=[1,-1]\n",
        "x_opt, trajectory = newtons_method(f(x), x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CYz7r_NM4GJ",
        "outputId": "1940eed5-1df6-4b01-c000-270cb4d8f58c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1 \n",
            "x: [ 0.64644661 -0.64644661]\n",
            "-----\n",
            "iteration 2 \n",
            "x: [ 0.42017244 -0.42017244]\n",
            "-----\n",
            "iteration 3 \n",
            "x: [ 0.27535697 -0.27535697]\n",
            "-----\n",
            "iteration 4 \n",
            "x: [ 0.18267507 -0.18267507]\n",
            "-----\n",
            "iteration 5 \n",
            "x: [ 0.12335865 -0.12335865]\n",
            "-----\n",
            "iteration 6 \n",
            "x: [ 0.07590552 -0.07590552]\n",
            "-----\n",
            "iteration 7 \n",
            "x: [ 0.05160952 -0.05160952]\n",
            "-----\n",
            "minimal solution :  [ 0.03217271 -0.03217271]\n",
            "iteration 8 \n",
            "x: [ 0.03217271 -0.03217271]\n",
            "-----\n"
          ]
        }
      ]
    }
  ]
}